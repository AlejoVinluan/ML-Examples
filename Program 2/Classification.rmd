Alejo Vinluan & Dinesh Angadipeta
abv210001 & dxa190032
CS 4375.004
02/18/2023

Classification Notebook

Linear models for classification work by finding a linear relationship between vectors of data.
With the vectors of data, the program can estimate where testing data fits in the model. 
There would be data X that can be used to predict data Y. For example, there may be a trend 
where incomes of $200,000 and up are most likely to come from employees that have Masters degrees. 
In the context of classification, we could use linear models to determine whether or not a person 
could be a good candidate for a loan or not.

The linear models can be good for classification since it is estimating where the test data falls
within the provided full set of data. The example within the textbook is based on women's height
and weight. If we added another column with 'Human' or 'Alien' and had Alien rows that suggested
Aliens are generally over 7 foot and 275 pounds, then the model would work well for predicting
whether or not the test data given is Human or Alien. Furthermore, linear regression is simple
and works well for larger datasets.

Using linear models for classification may not work well since linear regression is for regression
tasks rather than classification. Classification models don't use RSS. Instead, it uses counts of
classes in regions. Linear regression is good for predicting Y from X rather than if the Y falls
into a certain range, given X.

This code will set the working directory to "Program 2", then reading the provided CSV
and putting in variable "ford_listings". This dataset gives a model, year, price, transmission,
mileage, fuelType, tax, mpg, and engineSizes for Fords sold in the UK. 
```{r}
setwd("Program 2")
ford_listings <- read.csv("./data/ford.csv")
```

This code will divide the set into training set and test set. The "eighty" variable will take
a sample of eighty percent of the dataset. Then, the data will be split into "training_data" and
"testing_data". 80% of the data will be for training and the remaining will be for testing.
```{r}
eighty <- sample(1:nrow(ford_listings), nrow(ford_listings)*0.8, replace=FALSE)
training_data  <- ford_listings[eighty, ]
testing_data   <- ford_listings[-eighty, ]
```

Here are 5 examples of R functions being used for data exploration.
- head: View the first X rows of the given data
- summary: View a quick summary of the testing data
- mean: View the mean price of every used Ford sold in UK
- median: View the median mileage of every used Ford sold in UK
- sum: View the total tax price of all used Fords within the data
```{r}
head(training_data, 5)
summary(training_data)
mean(training_data$price)
median(training_data$mileage)
sum(training_data$tax)
```

This is how to create a scatter plot that compares a cars mileage to its price.
```{r}
plot(training_data$mileage, training_data$price, xlab = "Mileage", ylab = "Price") #nolint
```

Here is another example of a visual graph which is a histogram of a car's MPG.
```{r}
hist(training_data$mpg)
```

This chunk of code runs a logistic regression model by finding the vehicle's model
based on it's year, price, transmission, mileage, fuel type, tax, mpg, and fuel size.
A summary is then printed about the regression model.
```{r}
set.seed(1234)
regression_model <- glm(training_data$model ~ ., data = training_data, family = binomial(link = "logit")) # nolint
summary(regression_model)
```
The summary states that the year, price, transmission, mileage, tax, and mpg are good
indicators to find the whether the vehicle is a Fiesta. This is most likely because a vehicle's
price and tax (which go hand in hand) will generally be the same of all vehicle models
with similair mileage. Furthermore, mpg should be the same for vehicles of the
same model.

This chunk of code runs a naive Bayes model. After the model is run, it predicts a car's
model based on it's year, price, transmission, mileage, fuel type, tax, mpg, and engine size.
Accuracy is then calculated by comparing the prediction model's guesses with the actual
information within the testing data.
```{r}
library(e1071)
bayes_model <- naiveBayes(as.factor(training_data$model)~., data = training_data, type="class") # nolint
prediction_model <- predict(bayes_model, newdata = testing_data)
accuracy <- mean(prediction_model == testing_data$model)
accuracy
```
In this instance, the prediction model only had an accuracy of 44%.
